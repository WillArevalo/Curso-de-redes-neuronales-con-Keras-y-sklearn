# Optimizadores en redes neuronales

Los optimizadores permiten un paso (learning rate hacia una direccion mas eficiente) en busca de minimizar el error

Los optimizador base por defecto es el descenso de gradiente estocastico(SGD Stochastic Gradient Descent)

El mas utilizado es ADAM

Otros

- ADADELTA
- ADAGRAM
- RMSPROP
- MOMENTUM

![Plot_optimizers](https://mlfromscratch.com/content/images/2019/12/saddle.gif)